import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import torch.nn as nn
import torchvision
import torch.utils.data
import torchvision.transforms as transforms
import time
import os
import torch
from torch.autograd import Variable
import torch.nn.functional as F
from os import path

notebooks_dir_name = 'notebooks/'
notebooks_base_dir = path.join('./MyDrive/test_env/', notebooks_dir_name)

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

transform = transforms.Compose(
    [transforms.Resize(256),
     transforms.RandomHorizontalFlip(),
     transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
     ])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform,
                                        download=True)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform,
                                       download=True)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=False)
testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

from torch.utils.data import Subset


def train_valid_split(dl, val_split=0.25):
    total_items = dl.dataset.data.shape[0]
    idxs = np.random.permutation(total_items)
    train_idxs, valid_idxs = idxs[round(total_items * val_split):], idxs[:round(total_items * val_split)]

    train = Subset(dl, train_idxs)
    valid = Subset(dl, valid_idxs)
    return train, valid


train_dl, valid_dl = train_valid_split(trainloader)

dataiter = iter(trainloader)
images, labels = dataiter.next()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class AlexNet(nn.Module):
    def __init__(self, num_classes=10):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
           #1
            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            # 2
            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            # 3
            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            # 4
            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            # 5
            nn.Conv2d(384, 256, kernel_size=5, stride=1, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier= nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 6 * 6)
        x = self.classifier(x)
        return x


model = AlexNet(num_classes=10).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-4)

import datetime


def convert_seconds_format(n):
    return str(datetime.timedelta(seconds=n))


all_losses = []
all_valid_losses = []
print('training starting...')
start_time = time.time()

for epoch in range(10):
    epoch_start = time.time()
    model.train()
    running_loss = 0.0
    running_valid_loss = 0.0
    predictions = []
    total = 0
    correct = 0

    for i, data in enumerate(train_dl.dataset, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero parameter gradients
        optimizer.zero_grad()

        # forward + back optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    all_losses.append(running_loss / i)

    # evaluation mode

    model.eval()
    with torch.no_grad():
        for i, data in enumerate(valid_dl.dataset, 0):
            inputs, labels = data[0].to(device), data[1].to(device)
            outputs = model(inputs)
            valid_loss = criterion(outputs, labels)
            running_valid_loss += valid_loss.item()

            # the class with the highest score
            _, predicted = torch.max(outputs.data, 1)
            predictions.append(outputs)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    epoch_end = time.time()
    epoch_time = convert_seconds_format(epoch_end - epoch_start)
    all_valid_losses.append(valid_loss)
    print(f"epoch {epoch + 1}, running loss: {all_losses[-1]}")
    print(f"validation accuracy: {correct / total}. validation loss: {all_valid_losses[-1]}")
    print(f"epoch time: {epoch_time}")

torch.save(model.state_dict(), './test')
end_time = time.time()
train_time = convert_seconds_format(end_time - start_time)
print('training complete')
print(f"total time to train: {train_time}")
